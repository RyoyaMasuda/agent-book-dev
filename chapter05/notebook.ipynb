{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LangChain Expression Language（LCEL）徹底解説\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c9b716be-a79d-4f35-ab2d-8569e4f12df6-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# from google.colab import userdata\\\\\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\"\n",
    "\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = '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'\n",
    "\n",
    "# !export LANGCHAIN_TRACING_V2=true\n",
    "# !export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# !export LANGCHAIN_API_KEY=\"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\"\n",
    "# !export LANGCHAIN_PROJECT=\"agent-book\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-core==0.3.0\n",
      "  Downloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-openai==0.2.0\n",
      "  Downloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-community==0.3.0\n",
      "  Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain-core==0.3.0) (6.0.2)\n",
      "Collecting langsmith<0.2.0,>=0.1.117\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity!=8.4.0,<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/site-packages (from langchain-core==0.3.0) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/site-packages (from langchain-core==0.3.0) (4.12.2)\n",
      "Collecting pydantic<3.0.0,>=2.5.2\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai<2.0.0,>=1.40.0\n",
      "  Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain-community==0.3.0) (2.32.3)\n",
      "Collecting langchain<0.4.0,>=0.3.0\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting numpy<2,>=1\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (24.3.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (3.0.0)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3\n",
      "  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.0\n",
      "  Downloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.10-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.8-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.6-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.5-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.7.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.3.1)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.0) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.0) (2024.12.14)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.14.0)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0\n",
      "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl (27 kB)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "  Downloading langchain_text_splitters-0.3.1-py3-none-any.whl (25 kB)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tqdm, tenacity, regex, python-dotenv, pydantic-core, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpatch, jiter, greenlet, frozenlist, distro, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, pydantic, aiosignal, pydantic-settings, openai, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-4.0.3 dataclasses-json-0.6.7 distro-1.9.0 frozenlist-1.5.0 greenlet-3.1.1 jiter-0.8.2 jsonpatch-1.33 langchain-0.3.0 langchain-community-0.3.0 langchain-core-0.3.0 langchain-openai-0.2.0 langchain-text-splitters-0.3.0 langsmith-0.1.147 marshmallow-3.23.2 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 openai-1.58.1 orjson-3.10.12 propcache-0.2.1 pydantic-2.10.4 pydantic-core-2.27.2 pydantic-settings-2.7.0 python-dotenv-1.0.1 regex-2024.11.6 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.7.0 tqdm-4.67.1 typing-inspect-0.9.0 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 langchain-community==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Runnable と RunnableSequence―LCEL の最も基本的な構成要素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:12.290335Z",
     "iopub.status.busy": "2024-06-28T02:33:12.290156Z",
     "iopub.status.idle": "2024-06-28T02:33:12.344661Z",
     "shell.execute_reply": "2024-06-28T02:33:12.344241Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         ('system', 'ユーザーが入力した料理のレシピを考えてください。'),\n",
    "#         ('human', '{dish}')\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:12.346689Z",
     "iopub.status.busy": "2024-06-28T02:33:12.346510Z",
     "iopub.status.idle": "2024-06-28T02:33:21.108437Z",
     "shell.execute_reply": "2024-06-28T02:33:21.108007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カレーのレシピをご紹介します！以下は基本的なチキンカレーのレシピです。\n",
      "\n",
      "### 材料（4人分）\n",
      "- 鶏もも肉：400g（食べやすい大きさにカット）\n",
      "- 玉ねぎ：2個（みじん切り）\n",
      "- にんにく：2片（みじん切り）\n",
      "- 生姜：1片（みじん切り）\n",
      "- トマト：1個（ざく切り）\n",
      "- カレーパウダー：大さじ2\n",
      "- クミンシード：小さじ1\n",
      "- ココナッツミルク：200ml（お好みで）\n",
      "- サラダ油：大さじ2\n",
      "- 塩：適量\n",
      "- 黒胡椒：適量\n",
      "- 水：400ml\n",
      "- パクチー（飾り用）：適量\n",
      "\n",
      "### 作り方\n",
      "1. **下準備**: 鶏肉に塩と黒胡椒を振りかけて下味をつけておきます。\n",
      "\n",
      "2. **玉ねぎを炒める**: 大きめの鍋にサラダ油を熱し、みじん切りにした玉ねぎを加え、中火で透明になるまで炒めます。\n",
      "\n",
      "3. **香辛料を加える**: にんにく、生姜、クミンシードを加え、香りが立つまでさらに炒めます。\n",
      "\n",
      "4. **鶏肉を加える**: 鶏肉を鍋に加え、表面が白くなるまで炒めます。\n",
      "\n",
      "5. **トマトとカレーパウダーを加える**: ざく切りにしたトマトとカレーパウダーを加え、全体をよく混ぜます。\n",
      "\n",
      "6. **煮込む**: 水を加え、沸騰したら弱火にして蓋をし、約20分煮込みます。鶏肉が柔らかくなるまで煮込んでください。\n",
      "\n",
      "7. **ココナッツミルクを加える**: 最後にココナッツミルクを加え、さらに5分ほど煮込みます。味を見て、必要に応じて塩で調整します。\n",
      "\n",
      "8. **盛り付け**: お皿に盛り付け、パクチーを散らして完成です。\n",
      "\n",
      "### 提供方法\n",
      "ご飯やナンと一緒にお召し上がりください。お好みでヨーグルトやサラダを添えると、より一層美味しく楽しめます！\n",
      "\n",
      "ぜひお試しください！\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "output = output_parser.invoke(ai_message)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# prompt_value = prompt.invoke({'dish':'カレー'})\n",
    "# ai_message = model.invoke(prompt_value)\n",
    "# output = output_parser.invoke(ai_message)\n",
    "\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:21.110332Z",
     "iopub.status.busy": "2024-06-28T02:33:21.110173Z",
     "iopub.status.idle": "2024-06-28T02:33:21.112634Z",
     "shell.execute_reply": "2024-06-28T02:33:21.112238Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "# chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:21.114678Z",
     "iopub.status.busy": "2024-06-28T02:33:21.114418Z",
     "iopub.status.idle": "2024-06-28T02:33:26.539851Z",
     "shell.execute_reply": "2024-06-28T02:33:26.539250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
      "\n",
      "### 材料（4人分）\n",
      "- 鶏肉（もも肉または胸肉）: 400g\n",
      "- 玉ねぎ: 2個\n",
      "- にんじん: 1本\n",
      "- じゃがいも: 2個\n",
      "- カレールー: 1箱（約200g）\n",
      "- サラダ油: 大さじ2\n",
      "- 水: 800ml\n",
      "- 塩: 適量\n",
      "- 胡椒: 適量\n",
      "- お好みでガーリックパウダーや生姜: 適量\n",
      "\n",
      "### 作り方\n",
      "1. **材料の下ごしらえ**:\n",
      "   - 鶏肉は一口大に切り、塩と胡椒を振っておきます。\n",
      "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
      "\n",
      "2. **炒める**:\n",
      "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
      "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
      "\n",
      "3. **野菜を加える**:\n",
      "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
      "\n",
      "4. **煮る**:\n",
      "   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、中火にして蓋をし、約15分煮ます。\n",
      "\n",
      "5. **カレールーを加える**:\n",
      "   - 火を止めてカレールーを加え、よく溶かします。再び弱火にし、10分ほど煮込みます。お好みでガーリックパウダーや生姜を加えて風味を調整します。\n",
      "\n",
      "6. **仕上げ**:\n",
      "   - 味を見て、必要であれば塩で調整します。全体がなじんだら、火を止めます。\n",
      "\n",
      "7. **盛り付け**:\n",
      "   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\n",
      "\n",
      "### おすすめのトッピング\n",
      "- チーズ\n",
      "- 生卵（温泉卵や目玉焼き）\n",
      "- 青ねぎやパセリの刻んだもの\n",
      "\n",
      "この基本のカレーは、具材を変えたり、スパイスを追加したりすることでアレンジが可能です。お好みに合わせて楽しんでください！\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"dish\": \"カレー\"})\n",
    "print(output)\n",
    "\n",
    "# output = chain.invoke({'dish': 'カレー'})\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の実行方法―invoke・stream・batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:26.545129Z",
     "iopub.status.busy": "2024-06-28T02:33:26.544905Z",
     "iopub.status.idle": "2024-06-28T02:33:32.478679Z",
     "shell.execute_reply": "2024-06-28T02:33:32.478134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カレーのレシピをご紹介します。シンプルで美味しい基本的なチキンカレーの作り方です。\n",
      "\n",
      "### 材料（4人分）\n",
      "- 鶏もも肉：400g（大きめの一口大にカット）\n",
      "- 玉ねぎ：2個（みじん切り）\n",
      "- にんにく：2片（みじん切り）\n",
      "- 生姜：1片（みじん切り）\n",
      "- トマト：1個（ざく切り）\n",
      "- カレーパウダー：大さじ2\n",
      "- ココナッツミルク：200ml（または水）\n",
      "- サラダ油：大さじ2\n",
      "- 塩：適量\n",
      "- 黒胡椒：適量\n",
      "- パクチー（お好みで）：適量\n",
      "\n",
      "### 作り方\n",
      "1. **下ごしらえ**：\n",
      "   - 鶏もも肉に塩と黒胡椒をふりかけ、下味をつけておきます。\n",
      "\n",
      "2. **玉ねぎを炒める**：\n",
      "   - 大きめの鍋にサラダ油を熱し、みじん切りにした玉ねぎを加え、中火で透明になるまで炒めます。\n",
      "\n",
      "3. **香味野菜を加える**：\n",
      "   - にんにくと生姜を加え、香りが立つまでさらに炒めます。\n",
      "\n",
      "4. **鶏肉を加える**：\n",
      "   - 鶏もも肉を鍋に加え、表面が白くなるまで炒めます。\n",
      "\n",
      "5. **トマトとスパイスを加える**：\n",
      "   - ざく切りにしたトマトとカレーパウダーを加え、全体をよく混ぜます。トマトが崩れるまで炒めます。\n",
      "\n",
      "6. **煮込む**：\n",
      "   - ココナッツミルクを加え、全体を混ぜたら、蓋をして中弱火で約20分煮込みます。時々かき混ぜて、焦げ付かないように注意します。\n",
      "\n",
      "7. **味を調える**：\n",
      "   - 最後に塩で味を調整し、お好みでパクチーを散らして完成です。\n",
      "\n",
      "### 提供方法\n",
      "ご飯やナンと一緒に盛り付けて、お好みでヨーグルトやサラダを添えてお楽しみください。\n",
      "\n",
      "このレシピは基本的なチキンカレーですが、野菜や豆を加えたり、辛さを調整したりして、自分好みのアレンジを楽しんでください！"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# for chunk in chain.stream({'dish': 'カレー'}):\n",
    "#     print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:32.480592Z",
     "iopub.status.busy": "2024-06-28T02:33:32.480406Z",
     "iopub.status.idle": "2024-06-28T02:33:38.976064Z",
     "shell.execute_reply": "2024-06-28T02:33:38.974376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['カレーのレシピをご紹介します！以下は基本的なチキンカレーのレシピです。\\n\\n### 材料（4人分）\\n- 鶏もも肉：400g（食べやすい大きさにカット）\\n- 玉ねぎ：2個（みじん切り）\\n- にんにく：2片（みじん切り）\\n- 生姜：1片（みじん切り）\\n- トマト：1個（ざく切り）\\n- カレーパウダー：大さじ2\\n- クミンシード：小さじ1（お好みで）\\n- ココナッツミルク：200ml（お好みで）\\n- サラダ油：大さじ2\\n- 塩：適量\\n- 黒胡椒：適量\\n- 水：400ml\\n- パクチー（飾り用）：適量\\n\\n### 作り方\\n1. **下準備**: 鶏肉に塩と黒胡椒を振りかけて下味をつけておきます。\\n\\n2. **玉ねぎを炒める**: 大きめの鍋にサラダ油を熱し、みじん切りにした玉ねぎを加え、中火で透明になるまで炒めます。\\n\\n3. **香味野菜を加える**: にんにくと生姜を加え、香りが立つまでさらに炒めます。\\n\\n4. **鶏肉を加える**: 鶏肉を鍋に加え、表面が白くなるまで炒めます。\\n\\n5. **スパイスを加える**: カレーパウダーとクミンシードを加え、全体に絡めるように炒めます。\\n\\n6. **トマトと水を加える**: ざく切りにしたトマトと水を加え、煮立たせます。煮立ったら、弱火にして蓋をし、約20分煮込みます。\\n\\n7. **ココナッツミルクを加える**: ココナッツミルクを加え、さらに5分ほど煮込みます。味を見て、必要に応じて塩で調整します。\\n\\n8. **盛り付け**: お皿に盛り付け、パクチーを散らして完成です。\\n\\n### 提供方法\\nご飯やナンと一緒にお楽しみください。お好みでヨーグルトやサラダを添えると、より美味しくいただけます。\\n\\nぜひお試しください！', 'うどんのレシピをご紹介します。シンプルで美味しい「かけうどん」の作り方です。\\n\\n### 材料（2人分）\\n- うどん（乾麺または生麺）: 2玉\\n- だし汁: 600ml\\n  - だしの素: 大さじ1（または昆布と鰹節を使って自家製だしを取る）\\n  - 水: 600ml\\n- 醤油: 大さじ2\\n- みりん: 大さじ1\\n- 塩: 少々\\n- トッピング（お好みで）:\\n  - ネギ（小口切り）\\n  - 天かす\\n  - かまぼこ\\n  - ほうれん草やわかめ\\n  - すりごま\\n  - 生卵（温泉卵でも可）\\n\\n### 作り方\\n\\n1. **だしを取る**:\\n   - 鍋に水を入れ、だしの素を加えて中火にかけます。自家製だしを使う場合は、昆布を水に浸けておき、沸騰直前に鰹節を加え、火を止めて数分置いてからこします。\\n\\n2. **だし汁を味付け**:\\n   - だしが取れたら、醤油、みりん、塩を加えて味を整えます。軽く煮立たせて、味がなじむようにします。\\n\\n3. **うどんを茹でる**:\\n   - 別の鍋にたっぷりの水を沸かし、うどんをパッケージの指示に従って茹でます。茹で上がったら、冷水でしっかりと洗い、ぬめりを取ります。\\n\\n4. **盛り付け**:\\n   - 器に茹でたうどんを盛り、その上から温めただし汁を注ぎます。\\n\\n5. **トッピング**:\\n   - お好みのトッピングを加えます。ネギや天かす、かまぼこ、ほうれん草などを散らして、最後にすりごまや温泉卵をトッピングすると、さらに美味しくなります。\\n\\n6. **完成**:\\n   - すぐにお召し上がりください。温かいかけうどんは、寒い日にもぴったりです。\\n\\nお好みで、辛味大根や七味唐辛子を加えても美味しいです。ぜひお試しください！']\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "outputs = chain.batch([{\"dish\": \"カレー\"}, {\"dish\": \"うどん\"}])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カ\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL の「|」で様々な Runnable を連鎖させる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:38.984366Z",
     "iopub.status.busy": "2024-06-28T02:33:38.983620Z",
     "iopub.status.idle": "2024-06-28T02:33:39.072077Z",
     "shell.execute_reply": "2024-06-28T02:33:39.071585Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "# output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.073954Z",
     "iopub.status.busy": "2024-06-28T02:33:39.073805Z",
     "iopub.status.idle": "2024-06-28T02:33:39.076746Z",
     "shell.execute_reply": "2024-06-28T02:33:39.076291Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問にステップバイステップで回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser\n",
    "\n",
    "# cot_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         ('system', 'ユーザーの質問にステップバイステップで回答してください'),\n",
    "#         ('human', '{question}')\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# cot_chain = cot_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.078597Z",
     "iopub.status.busy": "2024-06-28T02:33:39.078287Z",
     "iopub.status.idle": "2024-06-28T02:33:39.080804Z",
     "shell.execute_reply": "2024-06-28T02:33:39.080464Z"
    }
   },
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser\n",
    "\n",
    "# sumarize_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         ('system', 'ステップバイステップで考えた回答から結論だけ抽出してください。'),\n",
    "#         ('human', '{text}')\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# summarize_chain = summarize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.082487Z",
     "iopub.status.busy": "2024-06-28T02:33:39.082344Z",
     "iopub.status.idle": "2024-06-28T02:33:42.144944Z",
     "shell.execute_reply": "2024-06-28T02:33:42.144538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
    "print(output)\n",
    "\n",
    "# cot_summarize_chain = cot_chain | summarize_chain\n",
    "# output = cot_summarize_chain.invoke({'question': '10+2*3'})\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. RunnableLambda―任意の関数を Runnable にする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:42.146898Z",
     "iopub.status.busy": "2024-06-28T02:33:42.146736Z",
     "iopub.status.idle": "2024-06-28T02:33:42.204154Z",
     "shell.execute_reply": "2024-06-28T02:33:42.203681Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:42.206082Z",
     "iopub.status.busy": "2024-06-28T02:33:42.205909Z",
     "iopub.status.idle": "2024-06-28T02:33:43.030226Z",
     "shell.execute_reply": "2024-06-28T02:33:43.029756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
    "\n",
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain デコレーターを使った RunnableLambda の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.032211Z",
     "iopub.status.busy": "2024-06-28T02:33:43.032048Z",
     "iopub.status.idle": "2024-06-28T02:33:43.501555Z",
     "shell.execute_reply": "2024-06-28T02:33:43.499283Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "\n",
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda への自動変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.508518Z",
     "iopub.status.busy": "2024-06-28T02:33:43.508321Z",
     "iopub.status.idle": "2024-06-28T02:33:43.511080Z",
     "shell.execute_reply": "2024-06-28T02:33:43.510672Z"
    }
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.512885Z",
     "iopub.status.busy": "2024-06-28T02:33:43.512594Z",
     "iopub.status.idle": "2024-06-28T02:33:43.961318Z",
     "shell.execute_reply": "2024-06-28T02:33:43.960803Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の入力の型と出力の型に注意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.963174Z",
     "iopub.status.busy": "2024-06-28T02:33:43.963026Z",
     "iopub.status.idle": "2024-06-28T02:33:43.965674Z",
     "shell.execute_reply": "2024-06-28T02:33:43.965305Z"
    }
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | upper\n",
    "\n",
    "# 以下のコードを実行するとエラーになります\n",
    "# output = chain.invoke({\"input\": \"Hello!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.967542Z",
     "iopub.status.busy": "2024-06-28T02:33:43.967246Z",
     "iopub.status.idle": "2024-06-28T02:33:44.531734Z",
     "shell.execute_reply": "2024-06-28T02:33:44.531210Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （コラム）独自の関数を stream に対応させたい場合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:44.533838Z",
     "iopub.status.busy": "2024-06-28T02:33:44.533678Z",
     "iopub.status.idle": "2024-06-28T02:33:45.353621Z",
     "shell.execute_reply": "2024-06-28T02:33:45.353115Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
    "    for text in input_stream:\n",
    "        yield text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | upper\n",
    "\n",
    "for chunk in chain.stream({\"input\": \"Hello!\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. RunnableParallel―複数の Runnable を並列で処理する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.355560Z",
     "iopub.status.busy": "2024-06-28T02:33:45.355402Z",
     "iopub.status.idle": "2024-06-28T02:33:45.407879Z",
     "shell.execute_reply": "2024-06-28T02:33:45.407407Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.409670Z",
     "iopub.status.busy": "2024-06-28T02:33:45.409524Z",
     "iopub.status.idle": "2024-06-28T02:33:45.412232Z",
     "shell.execute_reply": "2024-06-28T02:33:45.411889Z"
    }
   },
   "outputs": [],
   "source": [
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは楽観主義者です。ユーザーの入力に対して楽観的な意見をください。\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.414087Z",
     "iopub.status.busy": "2024-06-28T02:33:45.413807Z",
     "iopub.status.idle": "2024-06-28T02:33:45.416778Z",
     "shell.execute_reply": "2024-06-28T02:33:45.416359Z"
    }
   },
   "outputs": [],
   "source": [
    "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは悲観主義者です。ユーザーの入力に対して悲観的な意見をください。\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = pessimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.418636Z",
     "iopub.status.busy": "2024-06-28T02:33:45.418458Z",
     "iopub.status.idle": "2024-06-28T02:33:48.115947Z",
     "shell.execute_reply": "2024-06-28T02:33:48.115444Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "output = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel の出力を Runnable の入力に連結する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:48.117879Z",
     "iopub.status.busy": "2024-06-28T02:33:48.117728Z",
     "iopub.status.idle": "2024-06-28T02:33:54.979992Z",
     "shell.execute_reply": "2024-06-28T02:33:54.978363Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的AIです。2つの意見をまとめてください。\"),\n",
    "        (\"human\", \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic_opinion\": optimistic_chain,\n",
    "            \"pessimistic_opinion\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel への自動変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:54.986651Z",
     "iopub.status.busy": "2024-06-28T02:33:54.986098Z",
     "iopub.status.idle": "2024-06-28T02:33:54.994428Z",
     "shell.execute_reply": "2024-06-28T02:33:54.992735Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:55.000866Z",
     "iopub.status.busy": "2024-06-28T02:33:55.000312Z",
     "iopub.status.idle": "2024-06-28T02:34:01.170210Z",
     "shell.execute_reply": "2024-06-28T02:34:01.169705Z"
    }
   },
   "outputs": [],
   "source": [
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda との組み合わせ―itemgetter を使う例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:01.172136Z",
     "iopub.status.busy": "2024-06-28T02:34:01.171982Z",
     "iopub.status.idle": "2024-06-28T02:34:01.174756Z",
     "shell.execute_reply": "2024-06-28T02:34:01.174370Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "topic_getter = itemgetter(\"topic\")\n",
    "topic = topic_getter({\"topic\": \"生成AIの進化について\"})\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:01.176493Z",
     "iopub.status.busy": "2024-06-28T02:34:01.176278Z",
     "iopub.status.idle": "2024-06-28T02:34:09.682638Z",
     "shell.execute_reply": "2024-06-28T02:34:09.682146Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは客観的AIです。{topic}について2つの意見をまとめてください。\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "        \"topic\": itemgetter(\"topic\"),\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. RunnablePassthrough―入力をそのまま出力する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tavily-python==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "retriever = TavilySearchAPIRetriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"東京の今日の天気は？\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assign―RunnableParallel に値を追加する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:14.820735Z",
     "iopub.status.busy": "2024-06-28T02:34:14.820536Z",
     "iopub.status.idle": "2024-06-28T02:34:20.477887Z",
     "shell.execute_reply": "2024-06-28T02:34:20.477378Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
    "\n",
    "output = chain.invoke(\"東京の今日の天気は？\")\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:20.479839Z",
     "iopub.status.busy": "2024-06-28T02:34:20.479664Z",
     "iopub.status.idle": "2024-06-28T02:34:27.723756Z",
     "shell.execute_reply": "2024-06-28T02:34:27.723236Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(\"東京の今日の天気は？\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜補足：pick ＞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:27.725885Z",
     "iopub.status.busy": "2024-06-28T02:34:27.725708Z",
     "iopub.status.idle": "2024-06-28T02:34:34.101909Z",
     "shell.execute_reply": "2024-06-28T02:34:34.101439Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"context\": retriever,\n",
    "        }\n",
    "    )\n",
    "    .assign(answer=prompt | model | StrOutputParser())\n",
    "    .pick([\"context\", \"answer\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(\"東京の今日の天気は？\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （コラム）astream_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabでは次のコードの「async」の箇所に「Use of \"async\" not allowed outside of async function」と表示されますが、エラーなく実行できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:34.103973Z",
     "iopub.status.busy": "2024-06-28T02:34:34.103802Z",
     "iopub.status.idle": "2024-06-28T02:34:34.107769Z",
     "shell.execute_reply": "2024-06-28T02:34:34.107274Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for event in chain.astream_events(\"東京の今日の天気は？\", version=\"v2\"):\n",
    "    print(event, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in chain.astream_events(\"東京の今日の天気は？\", version=\"v2\"):\n",
    "    event_kind = event[\"event\"]\n",
    "\n",
    "    if event_kind == \"on_retriever_end\":\n",
    "        print(\"=== 検索結果 ===\")\n",
    "        documents = event[\"data\"][\"output\"]\n",
    "        for document in documents:\n",
    "            print(document)\n",
    "\n",
    "    elif event_kind == \"on_parser_start\":\n",
    "        print(\"=== 最終出力 ===\")\n",
    "\n",
    "    elif event_kind == \"on_parser_stream\":\n",
    "        chunk = event[\"data\"][\"chunk\"]\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （コラム）Chat history と Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "def respond(session_id: str, human_message: str) -> str:\n",
    "    chat_message_history = SQLChatMessageHistory(\n",
    "        session_id=session_id, connection=\"sqlite:///sqlite.db\"\n",
    "    )\n",
    "\n",
    "    ai_message = chain.invoke(\n",
    "        {\n",
    "            \"chat_history\": chat_message_history.get_messages(),\n",
    "            \"input\": human_message,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    chat_message_history.add_user_message(human_message)\n",
    "    chat_message_history.add_ai_message(ai_message)\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "session_id = uuid4().hex\n",
    "\n",
    "output1 = respond(\n",
    "    session_id=session_id,\n",
    "    human_message=\"こんにちは！私はジョンと言います！\",\n",
    ")\n",
    "print(output1)\n",
    "\n",
    "output2 = respond(\n",
    "    session_id=session_id,\n",
    "    human_message=\"私の名前が分かりますか？\",\n",
    ")\n",
    "print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
